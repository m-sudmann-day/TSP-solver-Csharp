cat("ERROR")
return(NULL)
}
# If the number of points is not too small to split the data set, and the level is not
# to exceed the maximum depth (counting split nodes only), ...
if (N >= minPoints * 2 && level <= depth)
{
# Call getBestSplit() to find the best split of the data into two separate regions,
# if any.
bestSplit <- getBestSplit(data, featureColumns, labelColumn, minPoints, costFncNum)
# If a best split was found that is better than our current single region...
if (!is.null(bestSplit))
{
# Recurse, treating both new regions as new trees, the only difference from a root
# node being that the level is incremented.
branch1 <- cTreeAux(bestSplit$set1, labelColumn, featureColumns, depth, minPoints, costFncNum, level+1)
branch2 <- cTreeAux(bestSplit$set2, labelColumn, featureColumns, depth, minPoints, costFncNum, level+1)
# Determine the depth of the deeper region/branch, and use that as the depth of the
# deepest desdendant of the current node.
greatestDepth <- max(branch1$depth, branch2$depth)
# Return a node with a node type (nodeType) of "split", the column to split on (col),
# the value to split on (value), the depth of the deepest descendant (depth), the level
# of this node (level), the number of training points in this region (points), and
# child nodes for the two sides of the split (branch1 and branch2).
return(list(nodeType="split", col=bestSplit$col, value=bestSplit$value,
depth=greatestDepth, level=level, points=N, branch1=branch1, branch2=branch2))
}
}
# If we get here, a split is not going to happen so we build a leaf node containing a
# node type (nodeType) of "leaf", a winning majority label (label), the probability of
# that label (prob), the depth of the deepest ancestor of this node which is the node
# itself (depth), the level of this node (level), and the number of training points in
# this region (N).
labels <- data[, labelColumn]
labelChoice <- getWinningLabel(labels)
return(list(nodeType="leaf", label=labelChoice$label, prob=labelChoice$prob,
depth=level, level=level, points=N))
}
#############################################################################
# Public function cTreePredict() generates classification predictions based on
# a tree returned from cTree() and test data.
#
# Parameters:
#   tree: a classification tree generated by cTree()
#   data: test data containing feature columns that match those of the training data
#
# Calls cTreePredictAux() to perform the recursive portion of the algorithm.
#
# Returns:
#   a data frame with two columns
#     predLabels: predicted labels
#     prob: the probability of each predicted label
cTreePredict <- function(tree, data)
{
labels <- c()
prob <- c()
# Loop through all rows in the test set and call cTreePredictAux() to do the recursive
# work of determining a prediction.  Store the prediction retrieved and its probability
# in a pair of vectors
if (nrow(data) > 0)
{
for(row in 1:nrow(data))
{
prediction <- cTreePredictAux(tree, data[row, ])
labels <- c(labels, prediction$predLabel)
prob <- c(prob, prediction$prob)
}
}
# Convert the labels and probability vectors into a data frame and return.
return(data.frame(predLabels=labels, prob=prob))
}
#############################################################################
# Private function cTreePredictAux() walks down the nodes of classification tree
# recursively based on a tree that was generated by cTree() (tree) and a row
# of a data frame (row).
#
# Returns a list containing the predicted label (predLabel) and its probability
# (prob).
cTreePredictAux <- function(tree, row)
{
# If tree is a leaf node, then we simply extract the predicted label that was
# determined when the tree was built, along with its probability and return.
if (tree$nodeType == "leaf")
{
return(list(predLabel=tree$label, prob=tree$prob))
}
# If tree is a split node.
else
{
# Pull the column referenced in the split node and get the corresponding value
# from the data row.
dataValue <- row[,tree$col]
# If the value is numeric, apply a comparison between the data value and the
# value stored in the split node.  Recurse to the first or second child of the
# tree node accordingly.
if (is.numeric(tree$value))
{
if (dataValue >= tree$value)
{
return(cTreePredictAux(tree$branch1, row))
}
else
{
return(cTreePredictAux(tree$branch2, row))
}
}
# For non-numeric values, the comparison is similar to the above, but instead uses
# an equality.  Recurse as above.
else if (dataValue == tree$value)
{
return(cTreePredictAux(tree$branch1, row))
}
else
{
return(cTreePredictAux(tree$branch2, row))
}
}
}
#############################################################################
# Private function splitData() splits a data.frame (data) into two sets based
# on the value (value) in a particular column (column).
#
# Returns a list containing the two sets, set1 and set2.
splitData <- function(data, column, value)
{
# Numeric columns that are not factors will be treated as continuous,
# and the split involves an inequality.
if (is.numeric(data[, column])) # int, double
{
set1Indexes <- (data[, column] >= value)
}
# Non-numeric and factor columns will be treated as discrete entries,
# and the split involves an equality, meaning that one set will contain
# one value, and the other set will contain all other values.
else # character, logical, factor
{
set1Indexes <- (data[,column] == value)
}
# set1Indexes contains logical values indicating whether a row should
# be placed in set1.  All others go to set 2.
set1 <- data[set1Indexes, ]
set2 <- data[!set1Indexes, ]
return(list(set1=set1, set2=set2))
}
#############################################################################
# Private function getWinningLabel() examines a list of discrete values and
# selects the majority value.  It returns a list containing the majority value
# (label) and its probability (prob).
getWinningLabel <- function(labels)
{
# The following code, which is probably much more efficient than what
# is actually used, was not working because it was returning strings
# for numeric factors.  I cannot reproduce this in isolation, but it
# is not operating as expected so I chose not to use it.
#counts <- as.data.frame(table(labels), stringsAsFactors=FALSE)
#pos <- which.max(counts[,2])
#label <- counts[pos,1]
#count <- counts[pos,2]
# Get the unique labels.
ul <- unique(labels)
# Count them.
counts <- rep(length(ul), 1)
for (i in 1:length(ul))
{
counts[i] <- sum(labels == ul[i])
}
# Find the position of the maximum count.  From that, get the majority
# label and its probability.
pos <- which.max(counts)
label <- ul[pos]
count <- counts[pos]
prob <- count/length(labels)
return(list(label=label, prob=prob))
}
#############################################################################
# Private function calculateCost() takes a vector of labels (labels) and
# applies a cost function (costFncNum) to generate a measure of the
# fragmentation of the labels.
calculateCost <- function(labels, costFncNum)
{
# Get a vector of the counts of each distinct label.
counts <- as.data.frame(table(labels), stringsAsFactors=FALSE)[,2]
# Normalize around the length of the vector to get a probability for each
# discrete value.  p becomes a vector of values in [0,1].
p <- counts/length(labels)
# Based on the cost function requested, apply the appropriate arithmetic.
if (costFncNum == costFnc.Entropy)
{
# Ignore labels that are not in the list because they produce an error
# and should not affect the entropy anyway.  This only happens because
# factors contain a list of all possible values, even those not present.
p <- p[p > 0]
return (-sum(p * log(p)))
} else if (costFncNum == costFnc.ME) {
return(1 - max(p))
} else if (costFncNum == costFnc.Gini) {
return(sum(p * (1-p)))
} else {
cat("Unrecognized cost function.  Please use Entropy, Gini, or ME")
}
}
#############################################################################
# Private function getBestSplit() takes a data frame (data), a list of the
# names of the feature columns (featureColumns), the name of the label
# column (labelColumn), the minimum number of points allowed in a region
# (minRowsInSet), and the cost function to apply (costFncNum), and
# finds the best possible split of the data frame that will reduce the
# fragmentation of the labels by the maximum amount possible.
# Returns a list of containing the
getBestSplit <- function(data, featureColumns, labelColumn, minRowsInSet, costFncNum)
{
N <- nrow(data)
# Do not perform any processing if there is no possibility of producing a split
# where both resulting sets are large enough.  This is a redundant check as the
# same logic is applied in cTreeAux().
if (N < minRowsInSet * 2)
{
return(NULL)
}
# Calculate the cost/fragmentation of the data in its current form,
# before the split.
currentCost <- calculateCost(data[,labelColumn], costFncNum)
bestSplit <- NULL
# For each unique value in each column...
for (col in featureColumns)
{
uniqueValues <- unique(data[, col])
for (value in uniqueValues)
{
# ...attempt a split.
sets <- splitData(data, col, value)
n1 <- nrow(sets$set1)
n2 <- nrow(sets$set2)
# If both halfs are larger than the minimum allowed region size...
if (n1 >= minRowsInSet && n2 >= minRowsInSet)
{
# Calculate the cost/fragmentation of both sets.
cost1 <- calculateCost(sets$set1[,labelColumn], costFncNum)
cost2 <- calculateCost(sets$set2[,labelColumn], costFncNum)
p <- n1/N
# Produce a score for the reduction in fragmentation caused by this split.
totalCost <- currentCost - p*cost1 - (1-p)*cost2
# If the score is not worse than the current situation, and its better
# than any of the candidates we've tried so far...
if (totalCost > 0 && (is.null(bestSplit) || totalCost > bestSplit$cost))
{
# Build a list to keep track of this current best split.
bestSplit <- list(cost=totalCost, col=col, value=value,
set1=sets$set1, set2=sets$set2)
}
}
}
}
# Eliminate the cost field because this has no meaning outside the function.
if (!is.null(bestSplit))
{
bestSplit$cost <- NULL
}
return(bestSplit)
}
#############################################################################
# cTree.R
#
# Matthew Sudmann-Day
# Barcelona GSE Data Science
#
# Performs a comparison of test and training errors between the classification
# tree algorithm in the script cTree.R and the library function party::ctree().
#
# When run directly, the script defines the above function, loads the 'spam'
# dataset, runs the function, and generates the results in a PDF.
#
# Uses R packages:
#   formula.tools
#   party
#   caTools
#   ggplot2
#
# Public functions: cTreeTest(), cTreeTestPlot()
#############################################################################
# Activate this code if you do not have the prerequisite libraries.
#install.packages("formula.tools")
#install.packages("party")
#install.packages("caTools")
#install.packages("ggplot2")
#############################################################################
# Public function cTreeTest() evaluates the test errors and training errors of
# this cTree algorithm, and compares it against party::ctree().  It loops through
# a number of maximum depths and produces a plot based on the errors measured.
#
# Parameters:
#   formula: an R-style formula to describe the label and the independent
#     columns in the data
#   data: a data frame containing the data from which both training and test
#     sets will be extracted
#   trainRatio: the portion of the data frame that should be dedicated to
#     training, the remainder being used for testing (default=0.7)
#   costFnc: the cost function to measure the fragmentation of the labels
#     (permitted values are "Entropy" (default), "ME", and "Gini") to be applied
#     to the cTree function only.  party::ctree() will use its own default.
#   minPoints: the minimum number of training observations permitted in a
#     single classification region (default=5).  This applies to cTree only.
#     party::cTree() will use its own default.
#
# Returns:  nothing
cTreeTest <- function(formula, data, trainRatio=0.7, costFnc="Entropy", minPoints=5)
{
require(formula.tools)
require(party)
require(caTools)
# Determine the label column.
labelColumn <- get.vars(lhs(formula))
# Split the data between training and test sets.
split <- sample.split(data, trainRatio)
train <- data[split,]
test <- data[!split,]
# Initialize a data frame for the results of the test.
results <- data.frame(depth=NA, actualDepth=NA, trainErrors=NA, testErrors=NA, comparisonTrainErrors=NA, comparisonTestErrors=NA)
# Loop through a number of maximum depths.
for (depth in (1:13))
{
# Train the tree on the training set.
tree <- cTree(formula, train, depth, minPoints, costFnc)
actualDepth <- tree$depth
# When we no longer use the authorized depth, there is no point in continuing to increase it.
if (actualDepth < depth)
{
break
}
# Generate predictions and calculate errors.
preds <- cTreePredict(tree, train)
trainErrors <- 1 - mean(train$spam == preds$predLabels)
preds <- cTreePredict(tree, test)
testErrors <- 1 - mean(test$spam == preds$predLabels)
# Train the party::ctree() class on the same training set.
comparisonTree <- ctree(formula, train, controls=ctree_control(maxdepth=depth))
# Generate predictions and calculate errors.
preds <- predict(comparisonTree, train)
comparisonTrainErrors <- 1 - mean(train$spam == round(preds))
preds <- predict(comparisonTree, test)
comparisonTestErrors <- 1 - mean(test$spam == round(preds))
# Append our results.
results <- rbind(results, c(depth, actualDepth, trainErrors, testErrors, comparisonTrainErrors, comparisonTestErrors))
}
results <- results[-1, ]
return(results)
}
#############################################################################
# Public function cTreeTestPlot() produces a plot based on the results of the
# cTreeTest() function.  It returns the plot and saves it to a PDF.
#
# Parameters:
#   results: a data frame containing the results of cTreeTestPlot()
#   file: the filename or path to which a PDF of the results will be written
#
# Returns: the plot
cTreeTestPlot <- function(results, file)
{
require(ggplot2)
data1 <- data.frame(depth=results$depth, errors=results$trainErrors, Category="cTree() Training")
data2 <- data.frame(depth=results$depth, errors=results$testErrors, Category="cTree() Testing")
data3 <- data.frame(depth=results$depth, errors=results$comparisonTrainErrors, Category="party.ctree() Training")
data4 <- data.frame(depth=results$depth, errors=results$comparisonTestErrors, Category="party.ctree() Testing")
data <- rbind(data1, data2, data3, data4)
plot <- ggplot(data=data)
plot <- plot + ggtitle("Depth of Classification Tree vs. Error Rates")
plot <- plot + geom_line(aes(x=depth, y=errors, color=Category))
plot <- plot + xlab("Tree Depth") + ylab("Errors")
ggsave(file, plot)
return(plot)
}
data <- data.frame(stringsAsFactors=FALSE, rbind(
c('slashdot','USA','yes',18,'None'),
c('google','France','yes',23,'Premium'),
c('digg','USA','yes',24,'Basic'),
c('kiwitobes','France','yes',23,'Basic'),
c('google','UK','no',21,'Premium'),
c('(direct)','New Zealand','no',12,'None'),
c('(direct)','UK','no',21,'Basic'),
c('google','USA','no',24,'Premium'),
c('slashdot','France','yes',19,'None'),
c('digg','USA','no',18,'None'),
c('google','UK','no',18,'None'),
c('kiwitobes','UK','no',19,'None'),
c('digg','New Zealand','yes',12,'Basic'),
c('slashdot','UK','no',21,'None'),
c('google','UK','yes',18,'Basic'),
c('kiwitobes','France','yes',19,'Basic')))
colnames(data) <- c('referer', 'country', 'read_faq', 'pages_visited', 'subscription')
View(cTree)
View(cTree)
cTree(subscription ~ ., data, 5, 2)
tree <- cTree(subscription ~ ., data, 5, 2)
results <- cTreePredict(tree, data)
results
mean(results$predLabels == data$subscription)
tree <- cTree(subscription ~ ., data, 5, 2, "ME")
results <- cTreePredict(tree, data)
mean(results$predLabels == data$subscription)
tree <- cTree(subscription ~ ., data, 5, 2, "Gini")
results <- cTreePredict(tree, data)
mean(results$predLabels == data$subscription)
setwd("C:/OneDrive/BGSE/SM&O/Problem Sets/PS2/Charting")
s <- "fifteen_1000_78.88_48.55265"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
library(ggplot2)
s <- "fifteen_1000_78.88_48.55265"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "uruguay_3_0.01_100919.8"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "uruguay_3_0.22_97589.35"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "qatar_3_0.01_11892.9"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "qatar_3_50.01_11497.28"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "qatar_3_50.01_11497.28"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "qatar_2_7.49_11128.44"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "uruguay_3_0.01_100919.8"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "uruguay_3_0.36_97589.35"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "uruguay_4_10.97_97607.41"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
s <- "uruguay_4_0.13_100919.8"
df <- read.csv(paste("../Output/", s, ".csv", sep=""), header=TRUE)
plot <- ggplot(data=df)
plot <- plot + geom_segment(aes(x=X1, y=Y1, xend=X2, yend=Y2))
plot <- plot + geom_point(aes(x=X1, y=Y1))
plot <- plot + geom_point(aes(x=X2, y=Y2))
plot <- plot + theme(panel.background=element_rect("white"), panel.border=element_rect("darkgray", fill=NA))
plot
ggsave(filename=paste("../Charting/", s, ".jpeg", sep=""), plot)
